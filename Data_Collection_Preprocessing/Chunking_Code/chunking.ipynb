{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping frame due to incorrect size.\n",
      "Detected 16 speech chunks\n",
      "Saved chunk 0 as output_chunks\\chunk_0.flac\n",
      "Saved chunk 1 as output_chunks\\chunk_1.flac\n",
      "Saved chunk 2 as output_chunks\\chunk_2.flac\n",
      "Saved chunk 3 as output_chunks\\chunk_3.flac\n",
      "Saved chunk 3 as output_chunks\\chunk_3.flac\n",
      "Saved chunk 4 as output_chunks\\chunk_4.flac\n",
      "Saved chunk 5 as output_chunks\\chunk_5.flac\n",
      "Saved chunk 6 as output_chunks\\chunk_6.flac\n",
      "Saved chunk 7 as output_chunks\\chunk_7.flac\n",
      "Saved chunk 7 as output_chunks\\chunk_7.flac\n",
      "Saved chunk 8 as output_chunks\\chunk_8.flac\n",
      "Saved chunk 8 as output_chunks\\chunk_8.flac\n",
      "Saved chunk 9 as output_chunks\\chunk_9.flac\n",
      "Saved chunk 10 as output_chunks\\chunk_10.flac\n",
      "Saved chunk 10 as output_chunks\\chunk_10.flac\n",
      "Saved chunk 11 as output_chunks\\chunk_11.flac\n",
      "Saved chunk 11 as output_chunks\\chunk_11.flac\n",
      "Saved chunk 12 as output_chunks\\chunk_12.flac\n",
      "Saved chunk 13 as output_chunks\\chunk_13.flac\n",
      "Saved chunk 14 as output_chunks\\chunk_14.flac\n",
      "Saved chunk 14 as output_chunks\\chunk_14.flac\n",
      "Chunk 15 is silent and will be skipped.\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import io\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import wave\n",
    "import os\n",
    "\n",
    "# Convert FLAC to raw audio data\n",
    "def flac_to_raw(flac_file):\n",
    "    audio = AudioSegment.from_file(flac_file, format=\"flac\")\n",
    "    audio = audio.set_channels(1)  # Convert to mono\n",
    "    audio = audio.set_sample_width(2)  # 16-bit samples\n",
    "    audio = audio.set_frame_rate(16000)  \n",
    "\n",
    "    raw_data = io.BytesIO()\n",
    "    audio.export(raw_data, format=\"wav\")\n",
    "    raw_data.seek(0)\n",
    "\n",
    "    with wave.open(raw_data, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        samples = wf.readframes(wf.getnframes())\n",
    "        samples = np.frombuffer(samples, dtype=np.int16)\n",
    "\n",
    "    return sample_rate, samples\n",
    "\n",
    "\n",
    "def vad_segment(samples, sample_rate, frame_duration_ms=30, padding_duration_ms=575, max_chunk_duration=9):\n",
    "    vad = webrtcvad.Vad(3)  # Mode 3 for more aggressive VAD\n",
    "    frame_size = int(sample_rate * frame_duration_ms / 1000)\n",
    "    max_chunk_samples = sample_rate * max_chunk_duration\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "\n",
    "    def frame_generator(samples, frame_size):\n",
    "        for start in range(0, len(samples), frame_size):\n",
    "            yield samples[start:start + frame_size]\n",
    "\n",
    "    def vad_collector(sample_rate, frames):\n",
    "        buffer = []\n",
    "        triggered = False\n",
    "        voiced_frames = []\n",
    "        silence_frames = 0\n",
    "        speech_segments = []\n",
    "\n",
    "        for frame in frames:\n",
    "            if len(frame) < frame_size:\n",
    "                print(\"Skipping frame due to incorrect size.\")\n",
    "                continue\n",
    "\n",
    "            frame_bytes = frame.tobytes()\n",
    "            try:\n",
    "                is_speech = vad.is_speech(frame_bytes, sample_rate)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame: {e}\")\n",
    "                continue\n",
    "\n",
    "            if is_speech:\n",
    "                if not triggered:\n",
    "                    triggered = True\n",
    "                    buffer.extend(voiced_frames)\n",
    "                    voiced_frames = []\n",
    "                buffer.append(frame)\n",
    "                silence_frames = 0\n",
    "            else:\n",
    "                if triggered:\n",
    "                    buffer.append(frame)\n",
    "                    silence_frames += 1\n",
    "                    if silence_frames > num_padding_frames or len(buffer) * frame_size >= max_chunk_samples:\n",
    "                        triggered = False\n",
    "                        speech_segments.append(np.concatenate(buffer))\n",
    "                        buffer = []\n",
    "                else:\n",
    "                    voiced_frames.append(frame)\n",
    "                    if len(voiced_frames) > num_padding_frames:\n",
    "                        voiced_frames = voiced_frames[1:]\n",
    "\n",
    "        if buffer:\n",
    "            speech_segments.append(np.concatenate(buffer))\n",
    "\n",
    "        print(f\"Detected {len(speech_segments)} speech chunks\")\n",
    "        return speech_segments\n",
    "\n",
    "    frames = frame_generator(samples, frame_size)\n",
    "    segments = vad_collector(sample_rate, frames)\n",
    "    return segments\n",
    "\n",
    "# Check if the audio segment contains significant speech\n",
    "def contains_significant_speech(audio_segment, silence_threshold_db=-40, min_duration_ms=1000):\n",
    "    duration_ms = len(audio_segment)\n",
    "    if duration_ms < min_duration_ms:\n",
    "        return False\n",
    "\n",
    "    avg_dBFS = audio_segment.dBFS\n",
    "    return avg_dBFS > silence_threshold_db\n",
    "\n",
    "# # Function to add silence padding to the beginning and end of audio segments\n",
    "# def add_silence_padding(audio_segment, duration_ms=575):\n",
    "#     silence = AudioSegment.silent(duration=duration_ms)\n",
    "#     return silence + audio_segment + silence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_chunks(chunks, sample_rate, output_dir='chunks', base_filename='chunk', max_duration_ms=9800, min_silence_db=-40):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk) > 0:\n",
    "            with io.BytesIO(chunk.tobytes()) as raw_data:\n",
    "                audio_segment = AudioSegment.from_raw(raw_data, sample_width=2, frame_rate=sample_rate, channels=1)\n",
    "\n",
    "                # # Add silence padding to the chunk\n",
    "                # audio_segment = add_silence_padding(audio_segment)\n",
    "\n",
    "                if not contains_significant_speech(audio_segment, silence_threshold_db=min_silence_db):\n",
    "                    print(f\"Chunk {i} is silent and will be skipped.\")\n",
    "                    continue\n",
    "\n",
    "                while len(audio_segment) > max_duration_ms:\n",
    "                    split_point = max_duration_ms\n",
    "\n",
    "                    # Find a silence point within a range to avoid cutting sentences\n",
    "                    for j in range(max_duration_ms - 1, max_duration_ms - 3000, -1):\n",
    "                        if audio_segment[j-1:j+1].dBFS < min_silence_db:\n",
    "                            split_point = j\n",
    "                            break\n",
    "\n",
    "                    # Export the chunk up to the split point as FLAC\n",
    "                    chunk_to_export = audio_segment[:split_point]\n",
    "                    chunk_to_export.export(os.path.join(output_dir, f'{base_filename}_{i}.flac'), format='flac')\n",
    "                    print(f\"Saved chunk {i} as {os.path.join(output_dir, base_filename)}_{i}.flac\")\n",
    "                    i += 1\n",
    "                    \n",
    "                    # Move to the next segment\n",
    "                    audio_segment = audio_segment[split_point:]\n",
    "\n",
    "                if len(audio_segment) > 0:\n",
    "                    audio_segment.export(os.path.join(output_dir, f'{base_filename}_{i}.flac'), format='flac')\n",
    "                    print(f\"Saved chunk {i} as {os.path.join(output_dir, base_filename)}_{i}.flac\")\n",
    "\n",
    "\n",
    "\n",
    "def main(flac_file):\n",
    "    sample_rate, samples = flac_to_raw(flac_file)\n",
    "    speech_chunks = vad_segment(samples, sample_rate)\n",
    "    save_chunks(speech_chunks, sample_rate, output_dir='output_chunks')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flac_file = 'audio3_vocals.flac'  \n",
    "    main(flac_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
